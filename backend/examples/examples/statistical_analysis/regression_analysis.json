{
  "id": "statistical_analysis_3",
  "title": "回归分析",
  "description": "使用Python进行各种回归分析",
  "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 设置中文显示\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 生成示例数据\nnp.random.seed(42)\n\n# 1. 简单线性回归\n# 生成数据\nX = np.random.uniform(0, 10, 100).reshape(-1, 1)\ny = 2 * X.ravel() + 1 + np.random.normal(0, 1, 100)\n\n# 拟合模型\nlr = LinearRegression()\nlr.fit(X, y)\n\n# 预测\ny_pred = lr.predict(X)\n\n# 打印结果\nprint('简单线性回归结果：')\nprint(f'截距: {lr.intercept_:.4f}')\nprint(f'斜率: {lr.coef_[0]:.4f}')\nprint(f'R²分数: {r2_score(y, y_pred):.4f}')\n\n# 可视化\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, alpha=0.5)\nplt.plot(X, y_pred, color='red', label='回归线')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('简单线性回归')\nplt.legend()\nplt.show()\n\n# 2. 多元线性回归\n# 生成数据\nX_multi = np.random.uniform(0, 10, (100, 3))\ny_multi = 2 * X_multi[:, 0] + 3 * X_multi[:, 1] - X_multi[:, 2] + 5 + np.random.normal(0, 1, 100)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n\n# 拟合模型\nlr_multi = LinearRegression()\nlr_multi.fit(X_train, y_train)\n\n# 预测\ny_train_pred = lr_multi.predict(X_train)\ny_test_pred = lr_multi.predict(X_test)\n\n# 打印结果\nprint('\\n多元线性回归结果：')\nprint('系数：')\nfor i, coef in enumerate(lr_multi.coef_):\n    print(f'X{i+1}: {coef:.4f}')\nprint(f'截距: {lr_multi.intercept_:.4f}')\nprint(f'训练集R²分数: {r2_score(y_train, y_train_pred):.4f}')\nprint(f'测试集R²分数: {r2_score(y_test, y_test_pred):.4f}')\n\n# 3. 多项式回归\n# 生成非线性数据\nX_poly = np.random.uniform(-5, 5, 100).reshape(-1, 1)\ny_poly = 0.5 * X_poly.ravel()**2 - 2 * X_poly.ravel() + 1 + np.random.normal(0, 1, 100)\n\n# 创建多项式特征\npoly_features = PolynomialFeatures(degree=2)\nX_poly_features = poly_features.fit_transform(X_poly)\n\n# 拟合模型\nlr_poly = LinearRegression()\nlr_poly.fit(X_poly_features, y_poly)\n\n# 用于绘图的预测值\nX_plot = np.linspace(-5, 5, 100).reshape(-1, 1)\nX_plot_features = poly_features.transform(X_plot)\ny_plot_pred = lr_poly.predict(X_plot_features)\n\n# 打印结果\nprint('\\n多项式回归结果：')\nprint('系数：')\nfor i, coef in enumerate(lr_poly.coef_):\n    print(f'X^{i}: {coef:.4f}')\nprint(f'R²分数: {r2_score(y_poly, lr_poly.predict(X_poly_features)):.4f}')\n\n# 可视化\nplt.figure(figsize=(10, 6))\nplt.scatter(X_poly, y_poly, alpha=0.5)\nplt.plot(X_plot, y_plot_pred, color='red', label='回归曲线')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('多项式回归')\nplt.legend()\nplt.show()\n\n# 4. 正则化回归\n# 生成高维数据\nX_reg = np.random.normal(0, 1, (100, 10))\ny_reg = X_reg[:, 0] + 2 * X_reg[:, 1] + np.random.normal(0, 0.1, 100)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\n# 拟合不同的模型\nmodels = {\n    '普通线性回归': LinearRegression(),\n    'Ridge回归': Ridge(alpha=1.0),\n    'Lasso回归': Lasso(alpha=1.0)\n}\n\n# 比较结果\nprint('\\n不同回归模型的比较：')\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    print(f'\\n{name}：')\n    print(f'训练集R²分数: {r2_score(y_train, y_train_pred):.4f}')\n    print(f'测试集R²分数: {r2_score(y_test, y_test_pred):.4f}')\n    print('系数：', model.coef_.round(4))\n\n# 5. 残差分析\n# 使用简单线性回归的结果\nresiduals = y - y_pred\n\n# 残差图\nplt.figure(figsize=(12, 4))\n\n# 残差散点图\nplt.subplot(121)\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('预测值')\nplt.ylabel('残差')\nplt.title('残差散点图')\n\n# 残差QQ图\nplt.subplot(122)\nsns.histplot(residuals, kde=True)\nplt.xlabel('残差')\nplt.ylabel('频数')\nplt.title('残差分布图')\n\nplt.tight_layout()\nplt.show()",
  "category_id": "statistical_analysis",
  "tags": ["统计分析", "回归分析", "机器学习"],
  "use_count": 0,
  "created_at": "2024-01-23T00:00:00",
  "updated_at": "2024-01-23T00:00:00"
} 